{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTERING THE DATASET:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along this script we will get from an initial list of products provided by our client, to a final list (as per the names and ids present within the real data), which will be used to filter our initial data in order to get a smaller, more manageable file.\n",
    "\n",
    "This process will be divided in two main steps:\n",
    "\n",
    "- Check the names in our list with the descriptions present in our data, analyze them and select a final list\n",
    "\n",
    "- Use this list to filter our data and store the resulting information in a more small and convenient file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING THE LIST OF PRODUCTS FOR THE ANALYSIS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rearranging the data in a more convenient manner and doing some introductory analysis of the data, we now want to get down to work with our data.\n",
    "\n",
    "A list has been given to us of the 10 products that our clients found as more relevant to their business.\n",
    "\n",
    "What we want now is to check whether the names on the list correspond to certain uniques ids, or, as seen in the previous scripts, some conflict of unicity will arise between the id of our products and their descriptions.\n",
    "\n",
    "So, we are going to check our dataframe and select from it the ids and descriptions of our products that match the indications given in our clients list. With the lists (in reality, two dictionaries) of the ids and descriptions that match every product given to us, we will decide which are the more appropriate.\n",
    "\n",
    "Perhaps some guidance from our client would be needed at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miruiz/kschool-final-project/lib/python2.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Importing packages:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import math\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the search path of the file, the name and the separator:\n",
    "\n",
    "file_path = \"../../data/01_raw/\"\n",
    "file_name = \"b2-transactions.csv\" #\"b2-transactions_sample.csv\" \n",
    "exit_path = \"../../data/02_intermediate/\"\n",
    "\n",
    "filtered_file_name=\"c1-filtered_transactions.csv\"\n",
    "\n",
    "sep=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the list of products provided by the client\n",
    "list_of_products=['croissant',\n",
    "                  'croissant petit',\n",
    "                  'tarta mousse 3 chocolates',\n",
    "                  'tarta de manzana 2º',\n",
    "                  'palmera de chocolate'\n",
    "                  'tarta opera',\n",
    "                  'postre fresas y mascarpone',\n",
    "                  'milhojas frambuesa 2º',\n",
    "                  'tortel',\n",
    "                  'baguette']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We import the dataframe:\n",
    "df=pd.read_csv(file_path+file_name, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>description</th>\n",
       "      <th>order_date</th>\n",
       "      <th>section</th>\n",
       "      <th>store</th>\n",
       "      <th>units_ordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8177333</th>\n",
       "      <td>323.0</td>\n",
       "      <td>MILHOJA DE TOMATE BERENJENA Y QUESO</td>\n",
       "      <td>13/5/2015 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>JPUP</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279470</th>\n",
       "      <td>130.0</td>\n",
       "      <td>SOLETILLAS</td>\n",
       "      <td>9/1/2011 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>CzUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11616603</th>\n",
       "      <td>182.0</td>\n",
       "      <td>PALMERAS DE TRUFA</td>\n",
       "      <td>3/4/2017 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AeUP</td>\n",
       "      <td>0,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20517163</th>\n",
       "      <td>259.0</td>\n",
       "      <td>COCA DE PICADILLO DE IBÉRICO</td>\n",
       "      <td>2/4/2015 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AnUP</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19948206</th>\n",
       "      <td>610.0</td>\n",
       "      <td>MANZANA</td>\n",
       "      <td>4/9/2008 0:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>AnUP</td>\n",
       "      <td>1,00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          product_id                          description         order_date  \\\n",
       "8177333        323.0  MILHOJA DE TOMATE BERENJENA Y QUESO  13/5/2015 0:00:00   \n",
       "4279470        130.0                           SOLETILLAS   9/1/2011 0:00:00   \n",
       "11616603       182.0                    PALMERAS DE TRUFA   3/4/2017 0:00:00   \n",
       "20517163       259.0         COCA DE PICADILLO DE IBÉRICO   2/4/2015 0:00:00   \n",
       "19948206       610.0                              MANZANA   4/9/2008 0:00:00   \n",
       "\n",
       "          section store units_ordered  \n",
       "8177333         0  JPUP          1,00  \n",
       "4279470         0  CzUP          0,00  \n",
       "11616603        0  AeUP          0,00  \n",
       "20517163        0  AnUP          1,00  \n",
       "19948206        0  AnUP          1,00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalizing and aggregating description names\n",
    "\n",
    "Unfortunately, there is no convention for the description and one id could \n",
    "\n",
    "1. Normalize descriptions as much as possible using:\n",
    "    - Regex expressions \n",
    "    - Basic NLP for spell-checking.\n",
    "2. Create a normalization file with the following structure:\n",
    "    - Unique Product_id and normalized description\n",
    "    - Flag to indicate if the product is part of the given list, or not.  \n",
    "3. Finally review the list manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Normalizing description names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Null descriptions to 'no-description'\n",
    "df['description'].fillna('no-description', inplace = True)\n",
    "\n",
    "# Unique product descriptions\n",
    "df_descriptions_unique = pd.Series(df['description'].unique())\n",
    "\n",
    "# Most of the descriptions are in uppercase, however others are in lower:\n",
    "df_descriptions_normalized = df_descriptions_unique.str.lower()\n",
    "\n",
    "#replace non alfanumeric with space\n",
    "df_descriptions_normalized=df_descriptions_normalized.str.replace(r'[^0-9a-zA-Zº()ª]+', ' ') \n",
    "\n",
    "# We also notice that there are spacing issues at the begining, end of the description and between words:\n",
    "df_descriptions_normalized=df_descriptions_normalized.str.strip()\n",
    "\n",
    "# Remove multi-spacing\n",
    "df_descriptions_normalized=df_descriptions_normalized.str.replace(r' +', ' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>desc_normalized</th>\n",
       "      <th>desc_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>caja mini sandwich de 10 rac</td>\n",
       "      <td>Caja mini sandwich de 10 rac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10551</th>\n",
       "      <td>encargo tarta tres chocolates de 20 raciones e...</td>\n",
       "      <td>Encargo TARTA TRES CHOCOLATES DE 20 RACIONES E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>tarta quiche jamon york 6r</td>\n",
       "      <td>TARTA QUICHE JAMON YORK 6R/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14740</th>\n",
       "      <td>mousse 3 chocolates 3ºescrito en tarta abel 2</td>\n",
       "      <td>MOUSSE 3  CHOCOLATES 3ºESCRITO EN TARTA \"ABEL 2\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>postre milhojas frambuesa rectangular</td>\n",
       "      <td>POSTRE  MILHOJAS  FRAMBUESA   RECTANGULAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21668</th>\n",
       "      <td>postre mousse chocolate blanco con guindas ama...</td>\n",
       "      <td>POSTRE MOUSSE CHOCOLATE BLANCO  CON GUINDAS  A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13918</th>\n",
       "      <td>cartel de chocolate happy birthday cristina</td>\n",
       "      <td>CARTEL DE CHOCOLATE \" Happy Birthday Cristina\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35779</th>\n",
       "      <td>encargotarta milhojas de frambuesas del 2 feli...</td>\n",
       "      <td>EncargoTARTA MILHOJAS DE FRAMBUESAS DEL 2. FEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35887</th>\n",
       "      <td>tarta sacher 1º con anagrama</td>\n",
       "      <td>TARTA SACHER 1º con anagrama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>tarta quche de espinacas futbol</td>\n",
       "      <td>TARTA  QUCHE  DE  ESPINACAS  FUTBOL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         desc_normalized  \\\n",
       "2127                        caja mini sandwich de 10 rac   \n",
       "10551  encargo tarta tres chocolates de 20 raciones e...   \n",
       "3309                          tarta quiche jamon york 6r   \n",
       "14740      mousse 3 chocolates 3ºescrito en tarta abel 2   \n",
       "9348               postre milhojas frambuesa rectangular   \n",
       "21668  postre mousse chocolate blanco con guindas ama...   \n",
       "13918        cartel de chocolate happy birthday cristina   \n",
       "35779  encargotarta milhojas de frambuesas del 2 feli...   \n",
       "35887                       tarta sacher 1º con anagrama   \n",
       "4322                     tarta quche de espinacas futbol   \n",
       "\n",
       "                                           desc_original  \n",
       "2127                        Caja mini sandwich de 10 rac  \n",
       "10551  Encargo TARTA TRES CHOCOLATES DE 20 RACIONES E...  \n",
       "3309                         TARTA QUICHE JAMON YORK 6R/  \n",
       "14740   MOUSSE 3  CHOCOLATES 3ºESCRITO EN TARTA \"ABEL 2\"  \n",
       "9348           POSTRE  MILHOJAS  FRAMBUESA   RECTANGULAR  \n",
       "21668  POSTRE MOUSSE CHOCOLATE BLANCO  CON GUINDAS  A...  \n",
       "13918     CARTEL DE CHOCOLATE \" Happy Birthday Cristina\"  \n",
       "35779  EncargoTARTA MILHOJAS DE FRAMBUESAS DEL 2. FEL...  \n",
       "35887                       TARTA SACHER 1º con anagrama  \n",
       "4322                 TARTA  QUCHE  DE  ESPINACAS  FUTBOL  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict(desc_original = df_descriptions_unique, desc_normalized = df_descriptions_normalized)).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets gets get our hands dirty and apply some maths to calculate string distnace and finish cleaning all those messy product descriptions... This is what we are going to do:\n",
    "\n",
    "1. Create a dataset with pastry products by parsing the bakery catalogues, and other pastry websites. (this was done manually, by converting the pdf catalogues to txt using an external web. THe resulting file is named productos.txt)\n",
    "\n",
    "2. Following the indications from: https://medium.com/@hdezfloresmiguelangel/implementando-un-corrector-ortogr%C3%A1fico-en-python-utilizando-la-distancia-de-levenshtein-498ec0dd1105 create an spell-checker based on the products.txt dataset and the Levenshtein distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../../data/01_additional_data/productos.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tarta'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"trta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafe'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction(\"café\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! the it seems to work. Lets now apply it to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check (line):\n",
    "    \"Given a sentence, returns spell-checks word by word\"\n",
    "    if type(line) == str and len(line) > 0:\n",
    "        new = []\n",
    "        line = line.split(\" \")\n",
    "        for word in line:\n",
    "            if type(word) == str:\n",
    "                word = correction(word)\n",
    "            new.append(word)\n",
    "        return \" \".join(new)\n",
    "            \n",
    "    else:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution! the following line of code may take some time to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying spell_check to all the dataset\n",
    "df_descriptions_normalized = df_descriptions_normalized.apply(lambda line: spell_check(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now merge the normalized names back to the original file, and check how effective was this cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge = pd.DataFrame(dict(description = df_descriptions_unique, desc_normalized = df_descriptions_normalized))\n",
    "\n",
    "df_with_normalized_descriptions = pd.merge(df, to_merge, how='left', on = 'description').sort_values(by='order_date')\n",
    "df_with_normalized_descriptions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control merge size:\n",
    "if (df.shape[0] == df_with_normalized_descriptions.shape[0] ): \n",
    "    print(\"All OK. 'df' has the same size as 'df_with_normalized_descriptions' \")\n",
    "else:\n",
    "    print(\"ERROR. 'df' has NOT the same size as 'df_with_normalized_descriptions' \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking effectiveness of the data cleaning:\n",
    "unique_descriptions_raw = len(df['description'].unique())\n",
    "unique_descriptions_normalized = len(df_with_normalized_descriptions['desc_normalized'].unique())\n",
    "print('The product descritions were cleaned from {} unique names to {}.'.format(unique_descriptions_raw,unique_descriptions_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super effective..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file to the intermiady folder\n",
    "output_path_df_with_normalized_descriptions = exit_path + 'data_with_normalized_names.csv'\n",
    "df_with_normalized_descriptions.to_csv(output_path_df_with_normalized_descriptions, index = False, sep = ';' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Identifying product descriptions that the client wants us to predict\n",
    "\n",
    "It is time to create the file that will be manually reviewed.\n",
    "\n",
    "- First, we compare the normalized descriptions with the list of products provided with the client, and suggest matches using the library fuzzywuzzy\n",
    "- Second, we will use the results from the other analysis.\n",
    "- third, we will manually evaluate if the results are good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Using the library fuzzywuzzy to compare the product normalized descriptions with the list of products provided by the client and suggest a match, or alternatively - \"match-not-found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_desc_unique = pd.DataFrame(df_with_normalized_descriptions[\"desc_normalized\"].unique(), columns = ['desc_normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match (line, options = list_of_products):\n",
    "    \"Returns product match if the the calculated difference between strings is greater than 80, 'match-not-found' otherwise\"\n",
    "    if not(line is None) and type(line)== str:\n",
    "        highest = process.extractOne(line,list_of_products)\n",
    "        if not(highest is None) and highest[1] >80:\n",
    "            return highest[0]\n",
    "        else:\n",
    "            return 'match-not-found'\n",
    "    else:\n",
    "        return 'match-not-found'\n",
    "\n",
    "# Applying matching function to all product normalized descriptions\n",
    "df_normalized_desc_unique[\"target_names_fuzzywuuzy\"] = df_normalized_id_desc_unique[\"desc_normalized\"].apply(lambda line: find_match(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now evaluate how effectively did we match the normalized descriptions with the list that the client provided us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets review the effectiveness filtering by 'mousse '. The expected result is that all 'mousse 3 chocolates' match\n",
    "df_normalized_desc_unique[df_normalized_id_desc_unique['desc_normalized'].str.contains('mousse')].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see... its not actually very good, lets try something different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Using the results from the other analysis\n",
    "\n",
    "Lets now use the results from the manual analysis to see how efective the measure was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this matching is performed at id level, \n",
    "# lets create a new dataset with unique product_id, descriptions, and evaluate it:\n",
    "df_normalized_id_desc_unique = df_with_normalized_descriptions[[\"product_id\",'desc_normalized']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_products_matches={100: 'croissant', \n",
    "                  101: 'croissant',\n",
    "                  102: 'croissant',\n",
    "                  103: 'croissant petit',\n",
    "                  9999: 'tarta mousse 3 chocolates', # almost only for order, creating a new id for this product is suggested\n",
    "                  462: 'tarta de manzana 2º',\n",
    "                  182: 'palmera de chocolate', # palmeras: 140\n",
    "                  414: 'tarta opera', # 9999, for order, mostly. If included, creating a new id for this product is suggested\n",
    "                  4511:'postre fresas y mascarpone',\n",
    "                  459: 'milhojas frambuesa 2º',\n",
    "                  112: 'tortel',\n",
    "                  115: 'baguette'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_names_a(product_id, dict_of_products_matches= dict_of_products_matches):\n",
    "    'Returns match if the product_id is found within the given dict or, otherise \"match-not-found\"'\n",
    "    if not(product_id is None) and not(math.isnan(product_id)) and int(product_id)  in dict_of_products_matches:\n",
    "        return dict_of_products_matches[int(product_id)]\n",
    "    else:\n",
    "        return 'match-not-found'\n",
    "    \n",
    "df_normalized_id_desc_unique['target_names_manual_analysis']=df_normalized_id_desc_unique[\"product_id\"].apply(lambda line: target_names_a(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now check how effective this was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets review the effectiveness filtering by 'mousse '. The expected result is that all 'mousse 3 chocolates' match\n",
    "df_normalized_id_desc_unique[df_normalized_id_desc_unique['desc_normalized'].str.contains('mousse')].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not very good, since most of the 'mousse 3 chocolates' are unmatched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we need an better way to match the results. Lets try doing keywords filtering product by product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Review Product by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, lets create again a dataframe with unique descriptions\n",
    "unique_normalized_decriptions = pd.DataFrame(df_with_normalized_descriptions['desc_normalized'].unique(), columns = ['desc_normalized'])\n",
    "\n",
    "#And a empty list to add all the unitary analysis. It will be use to concatenate results.\n",
    "list_of_dfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Matching: milhojas de frambuesa 2º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milhojas = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('milhojas')]\n",
    "milhojas_frambuesa = milhojas[milhojas['desc_normalized'].str.contains('frambuesa')].copy()\n",
    "milhojas_frambuesa['target_names_C'] = 'milhojas frambuesa'\n",
    "list_of_dfs.append(milhojas_frambuesa)\n",
    "milhojas_frambuesa.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Matching: croissant petite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "croissant = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('croissant')].copy()\n",
    "croissant_petite = croissant[croissant['desc_normalized'].str.contains('petit')].copy()\n",
    "croissant_petite['target_names_prod_by_prod'] = 'croissant'\n",
    "list_of_dfs.append(croissant_petite)\n",
    "croissant_petite.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Matching: croissant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "croissant = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('croissant')].copy()\n",
    "croissant_simple = croissant[~croissant['desc_normalized'].str.contains('petit|tira|masa')].copy()\n",
    "croissant_simple['target_names_prod_by_prod'] = 'croissant'\n",
    "list_of_dfs.append(croissant_simple)\n",
    "croissant_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Matching: mousse tres chocolates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mousse = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('mousse')].copy()\n",
    "mousse_tres = mousse[mousse['desc_normalized'].str.contains('tres|3')].copy()\n",
    "mousse_tres_chocolates = mousse_tres[mousse_tres['desc_normalized'].str.contains('chocolate')].copy()\n",
    "mousse_tres_chocolates['target_names_prod_by_prod'] = 'mousse tres chocolates'\n",
    "list_of_dfs.append(mousse_tres_chocolates)\n",
    "mousse_tres_chocolates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Matching: tarta de manzana 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manzana = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('manzana')]\n",
    "manzana_tarta = manzana[manzana['desc_normalized'].str.contains('tarta')].copy()\n",
    "manzana_tarta_dos = manzana_tarta[manzana_tarta['desc_normalized'].str.contains('dos|2')].copy()\n",
    "manzana_tarta_dos['target_names_prod_by_prod'] = 'tarta de manzana'\n",
    "list_of_dfs.append(manzana_tarta_dos)\n",
    "manzana_tarta_dos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 Matching: palmera de chocolate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palmera = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('palmera')]\n",
    "palmera_chocolate = palmera[palmera['desc_normalized'].str.contains('chocolate|trufa')].copy()\n",
    "palmera_chocolate['target_names_prod_by_prod'] = 'palmera chocolate'\n",
    "\n",
    "list_of_dfs.append(palmera_chocolate)\n",
    "palmera_chocolate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7 Matching: tarta ópera "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opera = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('opera')]\n",
    "opera_tarta = opera[opera['desc_normalized'].str.contains('tarta')].copy()\n",
    "opera_tarta['target_names_prod_by_prod'] = 'tarta opera'\n",
    "\n",
    "list_of_dfs.append(opera_tarta)\n",
    "opera_tarta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.9 Matching: postre de fresas y mascarpone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postre = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('postre')]\n",
    "postre_fresa = postre[postre['desc_normalized'].str.contains('fresa')].copy()\n",
    "postre_fresa_mascarpone = postre_fresa[postre_fresa['desc_normalized'].str.contains('mascarpone')].copy()\n",
    "\n",
    "postre_fresa_mascarpone['target_names_prod_by_prod'] = 'postre de fresas y mascarpone'\n",
    "list_of_dfs.append(postre_fresa_mascarpone)\n",
    "postre_fresa_mascarpone.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.9 Matching: tortel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tortel = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('tortel')].copy()\n",
    "tortel['target_names_prod_by_prod'] = 'tortel'\n",
    "list_of_dfs.append(tortel)\n",
    "tortel.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.10 Matching: baguette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baguette = unique_normalized_decriptions[unique_normalized_decriptions['desc_normalized'].str.contains('baguette|baguete|baguet')].copy()\n",
    "baguette['target_names_prod_by_prod'] = 'baguette'\n",
    "list_of_dfs.append(baguette)\n",
    "baguette.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets now concatenate results, merge them back to the  full list of normalized descriptions and evaluate its effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets concatenate the results:\n",
    "list_of_products_df = pd.concat(list_of_dfs, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_normalezed_vs_prod_by_prod = pd.merge(df_with_normalized_descriptions, list_of_products_df[['desc_normalized','target_names_prod_by_prod']],how='left',on = 'desc_normalized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control merge size:\n",
    "if (df_with_normalized_descriptions.shape[0] == aux.shape[0] ): \n",
    "    print(\"All OK. 'df_with_normalized_descriptions' has the same size as 'df_with_normalized_descriptions' \")\n",
    "else:\n",
    "    print(\"ERROR. 'df' has NOT the same size as 'df_with_normalized_descriptions' \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now check how effective it was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the first 10 description names\n",
    "df_desc_normalezed_vs_prod_by_prod[['desc_normalized','target_names_prod_by_prod']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets now review the effectiveness filtering by 'mousse '. The expected result is that all 'mousse 3 chocolates' match\n",
    "df_desc_normalezed_vs_prod_by_prod.loc[df_desc_normalezed_vs_prod_by_prod['desc_normalized'].str.contains('mousse'),['desc_normalized','target_names_prod_by_prod'] ].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better! lets now check that the data integrity has not been compromised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Test that data has not been corrputed\n",
    "\n",
    "To test the integrity of the data, the original dataset should be the same as the last dataset without that we added, in other words, without the columns with the normalized descriptuons, and the target names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets check the size of both dataframes:\n",
    "print(\"Original dataset shape: {}\".format(df.shape))\n",
    "print(\"Resulting dataset shape: {}\".format(df_desc_normalezed_vs_prod_by_prod.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape looks good, we were expecting the resulting dataset to have to columns more. Lets now evauate if they are actually the same dataset if we remove the added columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting original columnsd from the resulting df\n",
    "df_result = df_desc_normalezed_vs_prod_by_prod.loc[:, df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets compare it with the original dataset, sorting them out in the same way:\n",
    "df_result_sorted = df_result.sort_values(by = ['order_date','store','description','product_id', 'units_ordered']).reset_index().drop('index', axis = 1)\n",
    "df_original_sorted = df.sort_values(by = ['order_date','store','description',  'product_id', 'units_ordered']).reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that they have the same columns, and are sorted using the same criteria, lets evaluate if they are the same:\n",
    "comparison_result = df_result_sorted.equals(df_original_sorted)\n",
    "\n",
    "if comparison_result == True:\n",
    "    print('OK - The original dataset is the similar to the resulting dataset')\n",
    "else:\n",
    "    print('ERROR - The original dataset are NOT found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Filter dataset to only include the products from the list provided by the client, and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kschool-final-project",
   "language": "python",
   "name": "kschool-final-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
